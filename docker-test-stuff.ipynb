{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'lstm_model.pickle'\n",
    "TORCH_MODEL_PATH = 'lstm_model_1.pyt'\n",
    "BUZZ_NUM_GUESSES = 10\n",
    "BUZZ_THRESHOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def guess_and_buzz(model, question_text) -> Tuple[str, bool]:\n",
    "    guesses = model.guess([question_text], BUZZ_NUM_GUESSES)[0]\n",
    "    scores = [guess[1] for guess in guesses]\n",
    "    buzz = scores[0] / sum(scores) >= BUZZ_THRESHOLD\n",
    "    return guesses[0][0], buzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_guess_and_buzz(model, questions) -> List[Tuple[str, bool]]:\n",
    "    question_guesses = model.guess(questions, BUZZ_NUM_GUESSES)\n",
    "    outputs = []\n",
    "    for guesses in question_guesses:\n",
    "        scores = [guess[1] for guess in guesses]\n",
    "        buzz = scores[0] / sum(scores) >= BUZZ_THRESHOLD\n",
    "        outputs.append((guesses[0][0], buzz))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QAmodel(nn.Module):\n",
    "    def __init__(self, n_classes,vocab, model1,word_embedding_dim=300, hidden_dim=100, train_vocab_embeddings=None):\n",
    "        super(QAmodel, self).__init__()\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.embedding = nn.Embedding(self.vocab_size, word_embedding_dim, padding_idx=0)\n",
    "\n",
    "        if train_vocab_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(train_vocab_embeddings))\n",
    "        else:\n",
    "            pretrained_weights = np.zeros((self.vocab_size, 300))\n",
    "\n",
    "            for i, word in enumerate(vocab):\n",
    "                try:\n",
    "                    pretrained_weights[i] = model1[word]\n",
    "                except:\n",
    "                    pretrained_weights[i] = model1['unk']\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weights))\n",
    "\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.lstm = nn.LSTM(word_embedding_dim, hidden_dim, num_layers=1,\n",
    "            bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.dense1 = nn.Linear(hidden_dim*2, hidden_dim*2)\n",
    "        self.output = nn.Linear(hidden_dim*2, n_classes)\n",
    "\n",
    "    def forward(self, questions, lens):\n",
    "        bsz, max_len = questions.size()\n",
    "        embeds = self.dropout(self.embedding(questions))\n",
    "\n",
    "        lens, indices = torch.sort(lens, 0, True)\n",
    "        _, (enc_hids, _) = self.lstm(pack(embeds[indices], lens.tolist(), batch_first=True))\n",
    "        enc_hids = torch.cat( (enc_hids[0], enc_hids[1]), 1)\n",
    "        _, _indices = torch.sort(indices, 0)\n",
    "        enc_hids = enc_hids[_indices]\n",
    "        dense_output = self.dense1(enc_hids)\n",
    "        output = self.output(dense_output)\n",
    "        return F.log_softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LstmGuesser:\n",
    "    def __init__(self, device='cpu'):\n",
    "        self.lstm_model = None\n",
    "        self.i_to_class = None\n",
    "        self.class_to_i = None\n",
    "        self.voc = None\n",
    "        self.ind2word = None\n",
    "        self.word2ind = None\n",
    "        self.device = device\n",
    "        self.glove_model = None\n",
    "\n",
    "    def train(self, training_data) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def question_preprocessing(self, questions: List[str]) -> List[List[str]]:\n",
    "        questions = preprocess.cleaning(questions, False)\n",
    "        questions = [q.split() for q in questions]\n",
    "        return questions\n",
    "\n",
    "    def postprocess_answer(self, ans):\n",
    "        \"Replaces spaces with _ in the answers. Ideally, shouldn't be necessary.\"\n",
    "        return '_'.join(ans.split())\n",
    "\n",
    "    def guess(self, questions: List[str], max_n_guesses: Optional[int]) -> List[List[Tuple[str, float]]]:\n",
    "        self.lstm_model.eval()\n",
    "        questions_split = self.question_preprocessing(questions)\n",
    "        input_questions = []\n",
    "        for q in questions_split:\n",
    "            input_questions.append(self.vectorize_without_labels(q))\n",
    "\n",
    "        input_batch = self.batchify_without_labels(input_questions)\n",
    "        question_text = input_batch['text']\n",
    "        question_len = input_batch['len']\n",
    "        logits = self.lstm_model.forward(question_text, question_len).detach()\n",
    "        top_n, top_i = logits.topk(max_n_guesses)\n",
    "        answer_indices = top_i.numpy()\n",
    "        answer_scores = top_n.numpy()\n",
    "        answer_score_pair_lists = []\n",
    "        for i in range(len(answer_indices)):\n",
    "            q_top_answers = [self.postprocess_answer(self.i_to_class[ans_ind]) for ans_ind in answer_indices[i]]\n",
    "            q_top_scores = [score for score in answer_scores[i]]\n",
    "            answer_score_pair_lists.append( list(zip(q_top_answers, q_top_scores)) )\n",
    "\n",
    "        return answer_score_pair_lists\n",
    "\n",
    "\n",
    "    def batchify_without_labels(self, batch):\n",
    "        \"\"\"\n",
    "        Gather a batch of individual examples into one batch, \n",
    "        which includes the question text, question length and labels \n",
    "        Keyword arguments:\n",
    "        batch: list of outputs from vectorize function\n",
    "        \"\"\"\n",
    "\n",
    "        question_len = list()\n",
    "        for ex in batch:\n",
    "            question_len.append(len(ex))\n",
    "        x1 = torch.LongTensor(len(question_len), max(question_len)).zero_()\n",
    "        for i in range(len(question_len)):\n",
    "            question_text = batch[i]\n",
    "            vec = torch.LongTensor(question_text)\n",
    "            x1[i, :len(question_text)].copy_(vec)\n",
    "        q_batch = {'text': x1, 'len': torch.FloatTensor(question_len)}\n",
    "        return q_batch\n",
    "\n",
    "    def vectorize_with_labels(self, ex):\n",
    "        \"\"\"\n",
    "        vectorize a single example based on the word2ind dict. \n",
    "        Keyword arguments:\n",
    "        exs: list of input questions-type pairs\n",
    "        ex: tokenized question sentence (list)\n",
    "        label: type of question sentence\n",
    "        Output:  vectorized sentence(python list) and label(int)\n",
    "        e.g. ['text', 'test', 'is', 'fun'] -> [0, 2, 3, 4]\n",
    "        \"\"\"\n",
    "\n",
    "        question_text, question_label = ex\n",
    "        vec_text = [0] * len(question_text)\n",
    "\n",
    "        for i in range(len(question_text)):\n",
    "            try:\n",
    "                vec_text[i] = self.word2ind[question_text[i]]\n",
    "            except:\n",
    "                vec_text[i] = self.word2ind['<unk>']\n",
    "\n",
    "        return vec_text, question_label\n",
    "\n",
    "    def vectorize_without_labels(self, ex):\n",
    "        \"\"\"\n",
    "        vectorize a single example based on the word2ind dict. \n",
    "        Keyword arguments:\n",
    "        exs: list of input questions-type pairs\n",
    "        ex: tokenized question sentence (list)\n",
    "        label: type of question sentence\n",
    "        Output:  vectorized sentence(python list) and label(int)\n",
    "        e.g. ['text', 'test', 'is', 'fun'] -> [0, 2, 3, 4]\n",
    "        \"\"\"\n",
    "\n",
    "        question_text = ex\n",
    "        vec_text = [0] * len(question_text)\n",
    "\n",
    "        for i in range(len(question_text)):\n",
    "            try:\n",
    "                vec_text[i] = self.word2ind[question_text[i]]\n",
    "            except:\n",
    "                vec_text[i] = self.word2ind['<unk>']\n",
    "\n",
    "        return vec_text\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls):\n",
    "        with open(MODEL_PATH, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "            guesser = LstmGuesser()\n",
    "            guesser.i_to_class = params['i_to_class']\n",
    "            guesser.class_to_i = params['class_to_i']\n",
    "            guesser.voc = params['voc']\n",
    "            guesser.ind2word = params['ind2word']\n",
    "            guesser.word2ind = params['word2ind']\n",
    "            num_classes = params['num_classes']\n",
    "            guesser.glove_model = params['glove_model']\n",
    "            guesser.lstm_model = QAmodel(num_classes, guesser.voc, guesser.glove_model)\n",
    "            guesser.lstm_model.load_state_dict(torch.load(\n",
    "                TORCH_MODEL_PATH))\n",
    "            guesser.lstm_model.eval()\n",
    "        return guesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_app(enable_batch=True):\n",
    "    dan_guesser = LstmGuesser.load()\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    @app.route('/api/1.0/quizbowl/act', methods=['POST'])\n",
    "    def act():\n",
    "        question = request.json['text']\n",
    "        guess, buzz = guess_and_buzz(dan_guesser, question)\n",
    "        return jsonify({'guess': guess, 'buzz': True if buzz else False})\n",
    "\n",
    "    @app.route('/api/1.0/quizbowl/status', methods=['GET'])\n",
    "    def status():\n",
    "        return jsonify({\n",
    "            'batch': enable_batch,\n",
    "            'batch_size': 200,\n",
    "            'ready': True\n",
    "        })\n",
    "\n",
    "    @app.route('/api/1.0/quizbowl/batch_act', methods=['POST'])\n",
    "    def batch_act():\n",
    "        questions = [q['text'] for q in request.json['questions']]\n",
    "        return jsonify([\n",
    "            {'guess': guess, 'buzz': True if buzz else False}\n",
    "            for guess, buzz in batch_guess_and_buzz(dan_guesser, questions)\n",
    "        ])\n",
    "\n",
    "\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@click.group()\n",
    "def cli():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@cli.command()\n",
    "@click.option('--host', default='0.0.0.0')\n",
    "@click.option('--port', default=4861)\n",
    "@click.option('--disable-batch', default=False, is_flag=True)\n",
    "def web(host, port, disable_batch):\n",
    "    \"\"\"\n",
    "    Start web server wrapping lstm model\n",
    "    \"\"\"\n",
    "    print('lstm app running')\n",
    "    app = create_app(enable_batch=not disable_batch)\n",
    "    app.run(host=host, port=port, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@cli.command()\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Train the DAN model, requires downloaded data and saves to models/\n",
    "    \"\"\"\n",
    "    print(\"No training from within docker model. Train elsewhere and copy to the data directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@cli.command()\n",
    "def test_guess():\n",
    "    dan_guesser = LstmGuesser.load()\n",
    "    print(dan_guesser.guess(['This is a test question for ten points.'], 1))\n",
    "    print(dan_guesser.guess(['Here we have a first question for ten points.', 'And another question for ten points.'], 1))\n",
    "    print(dan_guesser.guess(['Here we have a first question for ten points.', 'And another question for ten points.'], 2))\n",
    "    print(guess_and_buzz(dan_guesser, 'This is a test question for ten points.'))\n",
    "    print(batch_guess_and_buzz(dan_guesser, ['This is a test question for ten points.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@cli.command()\n",
    "@click.option('--local-qanta-prefix', default='data/')\n",
    "def download(local_qanta_prefix):\n",
    "    \"\"\"\n",
    "    Run once to download qanta data to data/. Runs inside the docker container, but results save to host machine\n",
    "    \"\"\"\n",
    "    util.download(local_qanta_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
